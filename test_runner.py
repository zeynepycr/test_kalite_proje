"""
Test runner script - Ã–rnek test senaryolarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in
"""
import json
import os
from metrics import TestCaseEvaluator

def evaluate_test_file(test_file_path):
    """
    Test senaryosu dosyasÄ±nÄ± deÄŸerlendir
    
    Args:
        test_file_path: Test senaryosu JSON dosyasÄ± yolu
    """
    if not os.path.exists(test_file_path):
        print(f"âŒ Dosya bulunamadÄ±: {test_file_path}")
        return None
    
    with open(test_file_path, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    evaluator = TestCaseEvaluator()
    evaluation = evaluator.evaluate_test_cases(test_cases)
    
    print(f"\nğŸ“Š Test Senaryosu DeÄŸerlendirmesi: {test_file_path}")
    print("=" * 60)
    print(f"Toplam Test SayÄ±sÄ±: {evaluation['total_count']}")
    print(f"GeÃ§erli YapÄ±: {evaluation['valid_structure']} ({evaluation['valid_structure_percent']}%)")
    print(f"Ã–n KoÅŸul Var: {evaluation['has_prerequisites']} ({evaluation['has_prerequisites_percent']}%)")
    print(f"AdÄ±mlar Var: {evaluation['has_steps']} ({evaluation['has_steps_percent']}%)")
    print(f"Beklenen SonuÃ§: {evaluation['has_expected_result']} ({evaluation['has_expected_result_percent']}%)")
    print(f"Ortalama AdÄ±m UzunluÄŸu: {evaluation['avg_steps_length']}")
    print(f"Kalite Skoru: {evaluation['coverage_score']}%")
    print("=" * 60)
    
    return evaluation

if __name__ == "__main__":
    # Ã–rnek test dosyasÄ±nÄ± deÄŸerlendir
    example_file = "examples/example_manual_tests.json"
    if os.path.exists(example_file):
        evaluate_test_file(example_file)
    else:
        print(f"âš ï¸ Ã–rnek dosya bulunamadÄ±: {example_file}")

